
\section{Experiment}
\subsection{Phase 1.1 - Frozen Training}

Each network was trained ten times over the course of three hours resulting in a representatively distributed set of trained networks.
Figure \ref{fig:frozen_training_times} visualizes the relations between the average training times.

\input{fig/frozen_training_times}

No linear relation between parameter count and training time can be determined.
NasNet Large takes by far the longest to train with an average of approximately 3.5 minutes. 
MobileNet was the fastest to train with slightly under 30 seconds.


\input{fig/frozen_validation_accuracies}


Many networks hover around 90\% validation accuracy for their best case.
The earliest insight is that light-weight, small networks are up to par with bigger, much more complex implementations in the context of this experiment.
An additional amount of parameters does not seem to contribute to an increase in prediction accuracy for binary classifications like presentation attack detection.
Even smaller networks can get the same or a better average validation accuracy.

Inception Resnet V2 has a high accuracy fluctuation between training sessions which could not be attributed to any superficial property and the reason behind it is unclear.
On average the aforementioned network performed the worst while being the second largest.

After the training phase, the best-case networks were identified using the average validation accuracy.
The weights for each node are persisted and can be downloaded and applied to reproduce the predictions.
The following analysis will feature these persisted models.


\subsection{Phase 1.2 - All Layers Enabled Training}
All networks are trained again with the same configuration, but now all layers are able to adapt their parameters.
Immediately noticeable is the drastic increase in training time for each network.
Another important fact is the increase in resource consumption of unfrozen layers during the training phase.
The workstation was now crashing multiple times during training iterations with variations of out-of-memory exceptions.
Figure \ref{fig:liquid_training_times} visualizes the relations between the average training times once again.


\input{fig/liquid_training_times}


NasNet Larges training time increased tenfold and it takes almost 40 minutes on average.


\input{fig/liquid_validation_accuracies}


The increased time spent for a more thorough training phase was worth it as almost all networks now break the 90\% validation accuracy barrier.
Inception Resnet V2 delivers a respectable top accuracy after the disappointing result in the last section.


\input{tbl/training_diff}


Table \ref{tbl:training_diff} shows the relative gain in validation accuracy of frozen versus freely trained networks.
Many of the smaller networks do not gain much, but on the other hand Inception Resnet V2 and Xception are now much more competitive.
Xceptions best validation accuracy is competitive and comprabale with some of the algorithms handed in during the LivDet2017 competition at least in regards to the dataset this experiment uses. \cite{LIVDET}

\hfill

\subsection{Phase 2 - Prediction Analysis}
Serialized models are loaded and the testing dataset will be interpreted once again while capturing confidence values for further analysis.
The detection error trade-off curve confirms that none of the networks have a clear advantage compared to each other, but shows that NasNet Large and NasNet Mobile performed the worst with an average difference of TBD \% and TBD \% respectively.



\includegraphics[width=\linewidth]{det-all.jpg}


In the following subsections, each network will be inspected individually by evaluating the performance implicitly assuming a neutral FNMR and FMR rate balancing false positives and false negatives.
This allows the performance to be differentiated in terms of certain materials.


\newpage
\subsubsection{EfficientNet B0}
Even before unfreezing all layers EfficientNet B0 was providing a usable PAD detection accuracy, but only a small amount in comparison to the other networks.
Interestingly the the correct non-match rate increased by almost 6\% through the unfrozen training.

Prediction latencies are consistent, but the few outliers make the average time quite a lot higher than the median latency as seen in Table \ref{tbl:efficientnet_latencies}.

\predictiontables{EfficientNetB0}


\subsubsection{Inception Resnet V2}
A suprising increase in prediction accuracy of almost 30\% was observed for Inception Resnet V2.
The detection scores for all materials benefitted from the unfrozen training.
Liquid Ecoflex was already reliably detected by the network, but is now detected a bit more reliably.
The network gained significant detection  capabilities for all other materials.

An average latency of 60ms are slow compared to the other networks.

\predictiontables{InceptionResNetV2}


\subsubsection{MobileNet}
Not a lot improved for MobileNet by unfreezing the layers and in case of Gelatine some accuracy was lost.
Bona fide fingerprints were correctly detected with an accuracy of 91.7\% and 94.4\%, while attack presentation were detected correctly with 87.9 and 89.0\%.

With the relatively inaccurate detection comes the shortest latency with an average of 27ms.
MobileNet was the fastest network in this experiment to deliver a prediction result.

\predictiontables{MobileNet}


\subsubsection{Nasnet Large}
Liquid Ecoflex samples were correctly classified most of the time with a high average rate of 99.7\% after the unfrozen training.
The high attack presentation detection is unfortunately paired with a low correct match rate of only 80\% after unfrozen training, which is the lowest in the entire experiment.

With an average of almost 73ms Nasnet Large has the highest prediction latency.

\predictiontables{NASNetLarge}


\subsubsection{Resnet V2}
Liquid Ecoflex samples were again correctly classified most of the time with a similar rate of 99.1\% after the unfrozen training.
CMR and CNMR are more balanced and result in an overall better accuracy in comparison to Nasnet Large.
Additionally the prediction latency is on average half of Nasnet Larges.

\predictiontables{ResNet50V2}


\subsubsection{VGG16}
The unfrozen training resulted in a drastic decrease of bona fide fingerpints and compared to that only a minor increase in correct non-match rate.
VGG16 is the only network which lost accuracy after the unfrozen training.

\predictiontables{VGG16}


\subsubsection{Xception}
The best performer in the test has a correct non-match rate of 97.5\% and returns a prediction result in under 36ms which is quite fast as well in comparison to the other networks.
Presentation attack were able to be detected precicely with a max delta of under 1\%.

\predictiontables{Xception}




\endinput





\subsubsection{EfficientNet B0}

The only CMR over 90\% is achieved by EfficientNet B0 which is the second best performer over all.
Bona fide fingerprints were correctly detected with an accuracy of 92.5\%.

\predictiontables{efficientnet}



\medskip
Out of the three tested neural networks MobileNet was performing the best on average thanks to it's high true negative detection rate.
The other two networks however have a better true positive rate.
\bigskip\hrule



\subsection{MobileNet}
Bona fide fingerprints were correctly detected with an accuracy of 89.1\%, while presentation attacks were detected correctly with 93.19\%.
None of the materials show significant variance from each other and are within a range of 92.0\% and 94.7\%.
Liquid Ecoflex shows the worst deception potential.


\predictiontables{mobilenet}




\subsection{Nasnet Mobile}

With a CMR of 81.2\% Nasnet Mobile is the worst performer in the small network group.
The presentation attack detection for the Latex datasets was with only 63\% slightly better than randomly assigned outcomes.
A low precision in regards to the materials provide an interesting difference of over 20\% accuracy between Latex and Liquid Ecoflex.



\medskip
Out of the three tested neural networks MobileNet was performing the best on average thanks to it's high true negative detection rate.
The other two networks however have a better true positive rate.
\bigskip\hrule



\subsection{Xception}
    Presentation attack were able to be detected precicely with a max delta of 2.9\% and all accuracies are over 90\%.
    The overall performance is nothing outstanding and is in line with the median.




\subsection{Inception V3}

    The performance is very similar to the previous network with the accuracies differing by only 0.08\%.
    Inception V3s bona fide detection is a little better, but in turn resentation attacks a bit worse in comparison.




\subsection{EfficientNet B5}

    The highest correct non-match rate in the entire series is held by EfficientNet B5 with 96.03\% which is up to par with specialized solutions (cite livdet2017, p7).
    EfficientNet B5 is the best performer on average in the medium size category but the other two networks are very close in accuracy.

Networks in the midrange size deliver as strong performance and are precise in their accuracies.
Eï¬€icientNet B5 has the second best accuracy as well as the best CNMR.

\bigskip\hrule


\subsection{NASNet Large}
\begin{minipage}[c]{0.7\textwidth}
    More than a fifth of all predictions were incorrect which makes NASNet Large not suitable to enhance the quality of fingerprint presentation attack detection mechanisms.
    With almost 18.2\% of difference between Latex and Liquid Ecoflex, the precision is the worst among all tested networks.

    \medskip\centering Match Rates: 
    \begin{tabular}{ r  r  r  r |}
        CMR       & CNMR      & FNMR     & FMR     \\
        80.11\%   & 79.41\%   & 20.59\%  & 19.89\%  \\
    \end{tabular} \hspace{2mm} Accuracy: 79.73\%
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
    \centering
    \input{tbl/validation-nasnet-large.tex}
\end{minipage}



\subsection{VGG16}
\begin{minipage}[c]{0.7\textwidth}
    The second largest network in this test did not deliver any outstanding data.
    Accuracy and precision are certainly respectable and in the better half of all tested networks, but unremarkable considering the size and prediction latency.

    \medskip\centering Match Rates: 
    \begin{tabular}{ r  r  r  r |}
        CMR       & CNMR      & FNMR     & FMR     \\
        87.88\%   & 90.29\%   & 9.71\%   & 12.12\%  \\
    \end{tabular} \hspace{2mm} Accuracy: 89.19\%
\end{minipage}
\hfill
\begin{minipage}[c]{0.3\textwidth}
    \centering
    \input{tbl/validation-vgg16.tex}
\end{minipage}



\subsection{VGG19}
\begin{minipage}[c]{0.7\textwidth}
    The largest network provides solid non-match recognition, but cannot provice a good accuracy.
    A CNMR of almost 94\% is the second highest score comparable to algorithms which were handed in for LivDet2017.

    \medskip\centering Match Rates: 
    \begin{tabular}{ r  r  r  r |}
        CMR       & CNMR      & FNMR     & FMR     \\
        86.93\%   & 93.38\%   & 6.62\%   & 13.07\%  \\
    \end{tabular} \hspace{2mm} Accuracy: 90.45\%
\end{minipage}
\hfill
\begin{minipage}[c]{0.3\textwidth}
    \centering
    \input{tbl/validation-vgg19.tex}
\end{minipage}


Especially with regards to NASNet Large, the additional size seems to provide no benefit to fingerprint presentation attack-detection mechanisms.
For NASNet Large in particular, the additional size does not provide any benefit to fingerprint presentation attack-detection mechanisms.
VGG16 and VGG18 ware both marginally better than the average network and did not deliver the expected accuracy or precision.
